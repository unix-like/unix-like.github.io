[{"title":"tc命令——Linux基于IP进行流量限速","url":"/2017/05/28/service/tc命令Linux基于IP进行流量限速/","content":"\n# 一、TC原理\n\n　　Linux操作系统中的流量控制器TC（Traffic Control）用于Linux内核的流量控制，主要是通过在输出端口处建立一个队列来实现流量控制。\n\n<!-- more -->\n\n　　接收包从输入接口进来后，经过流量限制丢弃不符合规定的数据包，由输入多路分配器进行判断选择：\n\n* 如果接收包的目的主机是本主机，那么将该包送给上层处理，否则需要进行转发，将接收包交到转发块（Forwarding Block）处理。\n* 转发块同时也接收本主机上层(TCP、UDP等)产生的包，通过查看路由表，决定所处理包的下一跳。\n* 然后，对包进行排列以便将它们送到输出接口。\n\n　　一般只能限制网卡发送的数据包，不能限制网卡接收的数据包，所以可以通过改变发送次序靠控制传输速率。Linux流量控制主要是在输出接口排列时进行处理和实现的。\n\n# 二、TC规则\n\n## 2.1、流量控制方式\n\n　　流量控制包括以下几种方式：\n\n* SHAPING(限制) \n\n　　当流量被限制，它的传输速率就被控制在某个值以下。限制值可以大大小于有效带宽，这样可以平滑突发数据流量，使网络更为稳定。shaping（限制）只适用于向外的流量。\n\n* SCHEDULING(调度)      \n\n　　通过调度数据包的传输，可以在带宽范围内，按照优先级分配带宽。SCHEDULING(调度)也只适于向外的流量。\n\n* POLICING(策略)      \n\n　　SHAPING用于处理向外的流量，而POLICIING(策略)用于处理接收到的数据。\n\n* DROPPING(丢弃)      \n\n　　如果流量超过某个设定的带宽，就丢弃数据包，不管是向内还是向外。\n\n## 2.2、流量控制处理对象\n\n　　流量的处理由三种对象控制，它们是：\n\n* qdisc(排队规则)\n* class(类别)\n* filter(过滤器)\n\n###  QDISC(排队规则)\n\n　　QDisc(排队规则)是queueing discipline的简写，它是理解流量控制(traffic control)的基础。**无论何时，内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的qdisc(排队规则)把数据包加入队列。** 然后，内核会尽可能多地从qdisc里面取出数据包，把它们交给网络适配器驱动模块。最简单的QDisc是pfifo它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。不过，它会保存网络接口一时无法处理的数据包。\n\n　　QDISC的类别如下：\n\n#### （1）、CLASSLESS QDisc(不可分类QDisc)\n\n##### 1. 无类别QDISC包括：\n\n　　**[p|b]fifo**\n\n　　使用最简单的qdisc，纯粹的先进先出。只有一个参数：limit，用来设置队列的长度,pfifo是以数据包的个数为单位；bfifo是以字节数为单位。\n\n　　**pfifo_fast**\n\n　　在编译内核时，如果打开了高级路由器(Advanced Router)编译选项，pfifo_fast就是系统的标准QDISC。它的队列包括三个波段(band)。在每个波段里面，使用先进先出规则。而三个波段(band)的优先级也不相同，band 0的优先级最高，band 2的最低。如果band里面有数据包，系统就不会处理band 1里面的数据包，band 1和band 2之间也是一样。数据包是按照服务类型(Type of Service,TOS)被分配多三个波段(band)里面的。\n\n　　**red**\n\n　　red是Random Early Detection(随机早期探测)的简写。如果使用这种QDISC，当带宽的占用接近于规定的带宽时，系统会随机地丢弃一些数据包。它非常适合高带宽应用。\n\n　　**sfq**\n\n　　sfq是Stochastic Fairness Queueing的简写。它按照会话(session--对应于每个TCP连接或者UDP流)为流量进行排序，然后循环发送每个会话的数据包。\n\n　　**tbf**\n\n　　tbf是Token Bucket Filter的简写，适合于把流速降低到某个值。\n\n##### 2. 不可分类QDisc的配置\n\n　　如果没有可分类QDisc，不可分类QDisc只能附属于设备的根。它们的用法如下：\n\n```sh\ntc qdisc add dev DEV root QDISC QDISC-PARAMETERS\n```\n\n　　要删除一个不可分类QDisc，需要使用如下命令：\n\n```sh\ntc qdisc del dev DEV root\n```\n\n　　一个网络接口上如果没有设置QDisc，pfifo_fast就作为缺省的QDisc。\n\n#### （2）、CLASSFUL QDISC(分类QDisc)\n\n##### 可分类的QDisc包括：\n\n　　**CBQ**\n\n　　CBQ是Class Based Queueing(基于类别排队)的缩写。它实现了一个丰富的连接共享类别结构，既有限制(shaping)带宽的能力，也具有带宽优先级管理的能力。带宽限制是通过计算连接的空闲时间完成的。空闲时间的计算标准是数据包离队事件的频率和下层连接(数据链路层)的带宽。\n\n　　**HTB**\n\n　　HTB是Hierarchy Token Bucket的缩写。通过在实践基础上的改进，它实现了一个丰富的连接共享类别体系。使用HTB可以很容易地保证每个类别的带宽，虽然它也允许特定的类可以突破带宽上限，占用别的类的带宽。HTB可以通过TBF(Token Bucket Filter)实现带宽限制，也能够划分类别的优先级。\n\n　　**PRIO**\n\n　　PRIO QDisc不能限制带宽，因为属于不同类别的数据包是顺序离队的。使用PRIO QDisc可以很容易对流量进行优先级管理，只有属于高优先级类别的数据包全部发送完毕，才会发送属于低优先级类别的数据包。为了方便管理，需要使用iptables或者ipchains处理数据包的服务类型(Type Of Service,ToS)。\n\n### CLASS(类)       \n\n　　某些QDisc(排队规则)可以包含一些类别，不同的类别中可以包含更深入的QDisc(排队规则)，通过这些细分的QDisc还可以为进入的队列的数据包排队。通过设置各种类别数据包的离队次序，QDisc可以为设置网络数据流量的优先级。\n\n### FILTER(过滤器)      \n\n　　Filter(过滤器)用于为数据包分类，决定它们按照何种QDisc进入队列。无论何时数据包进入一个划分子类的类别中，都需要进行分类。分类的方法可以有多种，使用fileter(过滤器)就是其中之一。使用filter(过滤器)分类时，内核会调用附属于这个类(class)的所有过滤器，直到返回一个判决。如果没有判决返回，就作进一步的处理，而处理方式和QDISC有关。需要注意的是，filter(过滤器)是在QDisc内部，它们不能作为主体。\n\n## 2.3、操作原理\n\n　　类(Class)组成一个树，每个类都只有一个父类，而一个类可以有多个子类。某些QDisc(例如：CBQ和HTB)允许在运行时动态添加类，而其它的QDisc(例如：PRIO)不允许动态建立类。允许动态添加类的QDisc可以有零个或者多个子类，由它们为数据包排队。此外，每个类都有一个叶子QDisc，默认情况下，这个叶子QDisc使用pfifo的方式排队，我们也可以使用其它类型的QDisc代替这个默认的QDisc。而且，这个叶子叶子QDisc有可以分类，不过每个子类只能有一个叶子QDisc。 当一个数据包进入一个分类QDisc，它会被归入某个子类。\n\n　　我们可以使用以下三种方式为数据包归类，不过不是所有的QDisc都能够使用这三种方式：\n\n* tc过滤器(tc filter)\n\n　　如果过滤器附属于一个类，相关的指令就会对它们进行查询。过滤器能够匹配数据包头所有的域，也可以匹配由ipchains或者iptables做的标记。\n\n* 服务类型(Type of Service)\n\n　　某些QDisc有基于服务类型（Type of Service,ToS）的内置的规则为数据包分类。\n\n* skb->priority\n\n　　用户空间的应用程序可以使用SO_PRIORITY选项在skb->priority域设置一个类的ID。\n\n　　树的每个节点都可以有自己的过滤器，但是高层的过滤器也可以直接用于其子类。\n\n　　如果数据包没有被成功归类，就会被排到这个类的叶子QDisc的队中。相关细节在各个QDisc的手册页中。\n\n## 2.4、命名规则\n\n　　所有的QDisc、类和过滤器都有ID。ID可以手工设置，也可以有内核自动分配。ID由一个主序列号和一个从序列号组成，两个数字用一个冒号分开。\n\n　　**QDISC**\n\n　　一个QDisc会被分配一个主序列号，叫做句柄(handle)，然后把从序列号作为类的命名空间。句柄采用象10:一样的表达方式。习惯上，需要为有子类的QDisc显式地分配一个句柄。\n\n　　**类(CLASS)**\n\n　　在同一个QDisc里面的类分享这个QDisc的主序列号，但是每个类都有自己的从序列号，叫做类识别符(classid)。类识别符只与父QDisc有关，和父类无关。类的命名习惯和QDisc的相同。\n\n　　**过滤器(FILTER)**\n\n　　过滤器的ID有三部分，只有在对过滤器进行散列组织才会用到。详情请参考tc-filters手册页。\n\n## 2.5、单位\n\n　　tc命令的所有参数都可以使用浮点数，可能会涉及到以下计数单位。\n\n|带宽或者流速单位：\n|--------------------------------------------|\n|kbps                           |千字节/秒   |\n|mbps                           |兆字节/秒   |\n|kbit                           |KBits/秒    |\n|mbit                           |MBits/秒    |\n|bps或者一个无单位数字          |字节数/秒   |\n\n\n|数据的数量单位：\n|-------------------------------------|\n|kb或者k                        |千字节    |\n|mb或者m                        |兆字节    |\n|mbit                           |兆bit     |\n|kbit                           |千bit     |\n|b或者一个无单位数字            |字节数    |\n\n\n|时间的计量单位：\n|-------------------------------------------|\n|s、sec或者secs                    |秒    |\n|ms、msec或者msecs                 |分钟   |\n|us、usec、usecs或者一个无单位数字 |微秒  |\n\n\n# 三、TC命令\n\n　　tc可以使用以下命令对QDisc、类和过滤器进行操作：\n\n* add\n\n　　在一个节点里加入一个QDisc、类或者过滤器。添加时，需要传递一个祖先作为参数，传递参数时既可以使用ID也可以直接传递设备的根。如果要建立一个QDisc或者过滤器，可以使用句柄(handle)来命名；如果要建立一个类，可以使用类识别符(classid)来命名。\n\n* remove\n\n　　删除有某个句柄(handle)指定的QDisc，根QDisc(root)也可以删除。被删除QDisc上的所有子类以及附属于各个类的过滤器都会被自动删除。\n\n\n* change\n\n　　以替代的方式修改某些条目。除了句柄(handle)和祖先不能修改以外，change命令的语法和add命令相同。换句话说，change命令不能一定节点的位置。\n\n* replace\n\n　　对一个现有节点进行近于原子操作的删除/添加。如果节点不存在，这个命令就会建立节点。\n\n* link\n\n　　只适用于DQisc，替代一个现有的节点。\n\n\n# 四、具体操作\n\n　　Linux流量控制主要分为建立队列、建立分类和建立过滤器三个方面。\n\n## 4.1、基本实现步骤为：\n\n* （1） 针对网络物理设备（如以太网卡eth0）绑定一个队列QDisc；\n* （2） 在该队列上建立分类class；\n* （3） 为每一分类建立一个基于路由的过滤器filter；\n* （4） 最后与过滤器相配合，建立特定的路由表。\n\n## 4.2、环境模拟实例:\n\n　　流量控制器上的以太网卡(eth0) 的IP地址为192.168.1.66，在其上建立一个CBQ队列。假设包的平均大小为1000字节，包间隔发送单元的大小为8字节，可接收冲突的发送最长包数目为20字节。\n\n　　假如有三种类型的流量需要控制: \n\n1. 是发往主机1的，其IP地址为192.168.1.24。其流量带宽控制在8Mbit，优先级为2；\n2. 是发往主机2的，其IP地址为192.168.1.30。其流量带宽控制在1Mbit，优先级为1；\n3. 是发往子网1的，其子网号为192.168.1.0，子网掩码为255.255.255.0。流量带宽控制在1Mbit，优先级为6。\n\n\n### 1. 建立队列\n\n　　一般情况下，针对一个网卡只需建立一个队列。\n\n　　将一个cbq队列绑定到网络物理设备eth0上，其编号为1:0；网络物理设备eth0的实际带宽为10 Mbit，包的平均大小为1000字节；包间隔发送单元的大小为8字节，最小传输包大小为64字节。\n\n```sh\ntc qdisc add dev eth0 root handle 1: cbq bandwidth 10Mbit avpkt 1000 cell 8 mpu 64\n```\n\n### 2. 建立分类\n\n　　分类建立在队列之上。\n\n　　一般情况下，针对一个队列需建立一个根分类，然后再在其上建立子分类。对于分类，按其分类的编号顺序起作用，编号小的优先；一旦符合某个分类匹配规则，通过该分类发送数据包，则其后的分类不再起作用。\n\n　　**1） 创建根分类1:1；分配带宽为10Mbit，优先级别为8。**\n\n```sh\ntc class add dev eth0 parent 1:0 classid 1:1 cbq bandwidth 10Mbit rate 10Mbit maxburst 20 allot 1514 prio 8 avpkt 1000 cell 8 weight 1Mbit\n```\n\n　　该队列的最大可用带宽为10Mbit，实际分配的带宽为10Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为8，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为1Mbit。\n\n　　**2）创建分类1:2，其父分类为1:1，分配带宽为8Mbit，优先级别为2。**\n\n```sh\ntc class add dev eth0 parent 1:1 classid 1:2 cbq bandwidth 10Mbit rate 8Mbit maxburst 20 allot 1514 prio 2 avpkt 1000 cell 8 weight 800Kbit split 1:0 bounded\n```\n\n　　该队列的最大可用带宽为10Mbit，实际分配的带宽为 8Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为1，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为800Kbit，分类的分离点为1:0，且不可借用未使用带宽。\n\n　　**3）创建分类1:3，其父分类为1:1，分配带宽为1Mbit，优先级别为1。**\n\n```sh\ntc class add dev eth0 parent 1:1 classid 1:3 cbq bandwidth 10Mbit rate 1Mbit maxburst 20 allot 1514 prio 1 avpkt 1000 cell 8 weight 100Kbit split 1:0\n```\n\n　　该队列的最大可用带宽为10Mbit，实际分配的带宽为 1Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为2，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为100Kbit，分类的分离点为1:0。\n\n　　**4）创建分类1:4，其父分类为1:1，分配带宽为1Mbit，优先级别为6。**\n\n```sh\ntc class add dev eth0 parent 1:1 classid 1:4 cbq bandwidth 10Mbit rate 1Mbit maxburst 20 allot 1514 prio 6 avpkt 1000 cell 8 weight 100Kbit split 1:0\n```\n\n　　该队列的最大可用带宽为10Mbit，实际分配的带宽为1Mbit，可接收冲突的发送最长包数目为20字节；最大传输单元加MAC头的大小为1514字节，优先级别为6，包的平均大小为1000字节，包间隔发送单元的大小为8字节，相应于实际带宽的加权速率为100Kbit，分类的分离点为1:0。\n\n## 4.3. 建立过滤器 \n\n　　过滤器主要服务于分类。\n\n一般只需针对根分类提供一个过滤器，然后为每个子分类提供路由映射。\n\n**1） 应用路由分类器到cbq队列的根，父分类编号为1:0；过滤协议为ip，优先级别为100，过滤器为基于路由表。**\n\n```sh\ntc filter add dev eth0 parent 1:0 protocol ip prio 100 route\n```\n\n**2） 建立路由映射分类1:2, 1:3, 1:4**\n\n```sh\ntc filter add dev eth0 parent 1:0 protocol ip prio 100 route to 2 flowid 1:2\n\ntc filter add dev eth0 parent 1:0 protocol ip prio 100 route to 3 flowid 1:3\n\ntc filter add dev eth0 parent 1:0 protocol ip prio 100 route to 4 flowid 1:4\n```\n\n## 4.4.建立路由\n\n　　该路由是与前面所建立的路由映射一一对应。\n\n　　**1） 发往主机192.168.1.24的数据包通过分类2转发(分类2的速率8Mbit)**\n\n```sh\nip route add 192.168.1.24 dev eth0 via 192.168.1.66 realm 2\n```\n　　**2） 发往主机192.168.1.30的数据包通过分类3转发(分类3的速率1Mbit)**\n\n```sh\nip route add 192.168.1.30 dev eth0 via 192.168.1.66 realm 3\n```\n　　**3）发往子网192.168.1.0/24的数据包通过分类4转发(分类4的速率1Mbit)**\n\n```sh\nip route add 192.168.1.0/24 dev eth0 via 192.168.1.66 realm 4\n```\n\n　　**注**：一般对于流量控制器所直接连接的网段建议使用IP主机地址流量控制限制，不要使用子网流量控制限制。如一定需要对直连子网使用子网流量控制限制，则在建立该子网的路由映射前，需将原先由系统建立的路由删除，才可完成相应步骤。\n\n## 4.5. 监视\n\n　　主要包括对现有队列、分类、过滤器和路由的状况进行监视。\n\n　　**1）显示队列的状况**\n\n　　简单显示指定设备(这里为eth0)的队列状况\n\n```\ntc qdisc ls dev eth0\n\nqdisc cbq 1: rate 10Mbit (bounded,isolated) prio no-transmit\n```\n　　详细显示指定设备(这里为eth0)的队列状况\n\n```\ntc -s qdisc ls dev eth0\n```\n\n　　这里主要显示了通过该队列发送了13232个数据包，数据流量为7646731个字节，丢弃的包数目为0，超过速率限制的包数目为0。\n\n　　**2）显示分类的状况**\n\n　　简单显示指定设备(这里为eth0)的分类状况\n\n```\ntc class ls dev eth0\n```\n\n　　详细显示指定设备(这里为eth0)的分类状况\n\n```\ntc -s class ls dev eth0\n```\n\n　　这里主要显示了通过不同分类发送的数据包，数据流量，丢弃的包数目，超过速率限制的包数目等等。其中根分类(class cbq 1:0)的状况应与队列的状况类似。\n\n　　例如，分类class cbq 1:4发送了8076个数据包，数据流量为5552879个字节，丢弃的包数目为0，超过速率限制的包数目为0。\n\n　　**3）显示过滤器的状况**\n\n```\ntc -s filter ls dev eth0\n```\n　　这里flowid 1:2代表分类class cbq 1:2，to 2代表通过路由2发送。\n\n　　**4）显示现有路由的状况**\n\n```\nip route\n```\n\n　　如上所示，结尾包含有realm的显示行是起作用的路由过滤器。\n\n\n# 五、实例脚本\n\n## 5.1 tc限速\n\n```sh\n#! /bin/sh\n\ntouch  /var/lock/subsys/local\n\necho  1  > /proc/sys/net/ipv4/ip_forward （激活转发）\n\nroute add default  gw  10.0.0.0  (这是加入电信网关，如果你已设了不用这条）\n\nDOWNLOAD=640Kbit        (640/8 =80K ,我这里限制下载最高速度只能80K）\nUPLOAD=640Kbit          (640/8 =80K,上传速度也限制在80K）\nINET=192.168.0.         (设置网段，根据你的情况填）\nIPS=1                   (这个意思是从192.168.0.1开始）\nIPE=200                 (我这设置是从IP为192.168.0.1-200这个网段限速，根据自已的需要改）\nServerIP=253            (网关IP）\nIDEV=eth0\nODEV=eth1\n\n/sbin/tc  qdisc  del  dev  $IDEV root handle 10:\n/sbin/tc  qdisc  del  dev  $ODEV  root handle  20:\n/sbin/tc  qdisc  add  dev $IDEV  root  handle  10: cbq  bandwidth  100Mbit avpkt  1000\n/sbin/tc  qdisc  add  dev  $ODEV  root  handle  20: cbq bandwidth  1Mbit  avpkt  1000\n/sbin/tc  class  add  dev $IDEV  parent 10:0  classid  10:1  cbq  bandwidth  100Mbit  rate 100Mbit  allot 1514  weight  1Mbit  prio  8  maxburst  20  avpkt  1000\n/sbin/tc  class  add  dev  $ODEV  parent  20:0  classid  20:1 cbq  bandwidth  1Mbit  rate  1Mbit  allot  1514  weitht  10Kbit  prio  8  maxburst  20  avpkt  1000\n\nCOUNTER=$IPS\nwhile  [  $COUNTER  -le  $IPE  ]\n    do\n/sbin/tc  class  add  dev  $IDEV  parent  10:1  classid  10:1$COUNTER  cbq  banwidth  100Mbit  rate  \n$DOWNLOAD  allot  1514  weight  20Kbit  prio  5  maxburst  20  avpkt  1000  bounded\n/sbin/tc  qdisc  add  dev  $IDEV  parent  10:1$COUNTER  sfq  quantum  1514b  perturb15\n\n/sbin/tc  filter  add  dev  $IDEV  parent  10:0  protocol  ip  prio  100  u32  match  ipdst  $INET$COUNTER  flowid  10:1$COUNTER\n      COUNTER=` expr  $COUNTER  +  1  `\ndone\n\niptables  -t  nat  -A  POSTROUTING  -o  eth1  -s  192.168.0.0/24  -J  MASQUERADE\n```\n\n## 5.2 模型\n\n```sh   \n#!/bin/sh\ntc qdisc del dev eth7 root &> /dev/null\ntc qdisc del dev eth8 root &> /dev/null\n\n#Add qdisc\ntc qdisc add dev eth7 root handle 10: htb default 9998\ntc qdisc add dev eth8 root handle 10: htb default 9998\n\n#Add htb root node\ntc class add dev eth7 parent 10: classid 10:9999 htb rate 1000000kbit ceil 1000000kbit\ntc class add dev eth8 parent 10: classid 10:9999 htb rate 1000000kbit ceil 1000000kbit\n\n#Add htb fake default node here\ntc class add dev eth7 parent 10:9999 classid 10:9998 htb rate 1000000kbit ceil 1000000kbit\ntc class add dev eth8 parent 10:9999 classid 10:9998 htb rate 1000000kbit ceil 1000000kbit\n\n#Add rule node\ntc class add dev eth7 parent 10:9999 classid 10:3 htb rate 1kbit ceil 50kbit\ntc filter add dev eth7 parent 10: protocol ip handle 3 fw classid 10:3\ntc class add dev eth8 parent 10:9999 classid 10:3 htb rate 1kbit ceil 50kbit\ntc filter add dev eth8 parent 10: protocol ip handle 3 fw classid 10:3\n\n#Add htb real default node here\ntc class change dev eth7 classid 10:9998 htb rate 1kbit ceil 1000000kbit\ntc class change dev eth8 classid 10:9998 htb rate 1kbit ceil 1000000kbit\n```\n\n## 5.3 限制一个IP上传下载速度\n\n```sh\n#!/bin/bash\n#\n#  tc uses the following units when passed as a parameter.\n#  kbps: Kilobytes per second\n#  mbps: Megabytes per second\n#  kbit: Kilobits per second\n#  mbit: Megabits per second\n#  bps: Bytes per second\n#       Amounts of data can be specified in:\n#       kb or k: Kilobytes\n#       mb or m: Megabytes\n#       mbit: Megabits\n#       kbit: Kilobits\n#  To get the byte figure from bits, divide the number by 8 bit\n#\n\n#\n# Name of the traffic control command.\nTC=/sbin/tc\n\n# The network interface we're planning on limiting bandwidth.\nIF=em1             # Interface\n\n# Download limit (in mega bits)\nDNLD=80mbit          # DOWNLOAD Limit\n\n# Upload limit (in mega bits)\nUPLD=80mbit          # UPLOAD Limit\n\n# IP address of the machine we are controlling\nIP=125.64.15.21     # Host IP\n\n# Filter options for limiting the intended interface.\nU32=\"$TC filter add dev $IF protocol ip parent 1:0 prio 1 u32\"\n\nstart() {\n\n# We'll use Hierarchical Token Bucket (HTB) to shape bandwidth.\n# For detailed configuration options, please consult Linux man\n# page.\n\n    $TC qdisc add dev $IF root handle 1: htb default 30\n    $TC class add dev $IF parent 1: classid 1:1 htb rate $DNLD\n    $TC class add dev $IF parent 1: classid 1:2 htb rate $UPLD\n    $U32 match ip dst $IP/32 flowid 1:1\n    $U32 match ip src $IP/32 flowid 1:2\n\n# The first line creates the root qdisc, and the next two lines\n# create two child qdisc that are to be used to shape download\n# and upload bandwidth.\n#\n# The 4th and 5th line creates the filter to match the interface.\n# The 'dst' IP address is used to limit download speed, and the\n# 'src' IP address is used to limit upload speed.\n\n}\n\nstop() {\n\n# Stop the bandwidth shaping.\n    $TC qdisc del dev $IF root\n\n}\n\nrestart() {\n\n# Self-explanatory.\n    stop\n    sleep 1\n    start\n\n}\n\nshow() {\n\n# Display status of traffic control status.\n    $TC -s qdisc ls dev $IF\n\n}\n\ncase \"$1\" in\n\n  start)\n\n    echo -n \"Starting bandwidth shaping: \"\n    start\n    echo \"done\"\n    ;;\n\n  stop)\n\n    echo -n \"Stopping bandwidth shaping: \"\n    stop\n    echo \"done\"\n    ;;\n\n  restart)\n\n    echo -n \"Restarting bandwidth shaping: \"\n    restart\n    echo \"done\"\n    ;;\n\n  show)\n\n    echo \"Bandwidth shaping status for $IF:\"\n    show\n    echo \"\"\n    ;;\n\n  *)\n\n    pwd=$(pwd)\n    echo \"Usage: tc.bash {start|stop|restart|show}\"\n    ;;\n\nesac\n\nexit 0\n```\n\n本文原文出处:http://leslie-chu.blog.163.com/blog/static/19986324320125414618221\n\n主要参考（所有权利归原文作者所有）：\n\nhttp://www.cnblogs.com/endsock/archive/2011/12/09/2281519.html\n\nhttp://blog.163.com/ninja_wk/blog/static/989155620084280154811/\n\nhttp://www.chinaunix.net/jh/4/16110.html\n","tags":["tc"],"categories":["service"]},{"title":"Linux上ssd优化","url":"/2017/05/27/system/Linux上ssd优化/","content":"# 一、修改默认的固态硬盘(SSD)柱面大小\n\n　　提升Linux下固态硬盘的使用率，在安装Linux操作系统前就应该做相关工作。系统会先在磁盘上创建分区，通常创建的分区包含固定数量的柱面，而默认情况下，每个柱面由16065512个字节的扇区组成。\n  \n<!-- more -->\n\n　　现在的问题是，当默认柱面空间大小被完全使用后，固态硬盘就不能发挥最佳性能。因为要固态硬盘读这个操作需要使用4KB的字节块，而固态硬盘控制器删除操 作则需要512KB的字节块。问题是，有了通常用于Linux上的默认分区，分区的开始没必要也是一个4KB新分区的开始。结果，一次读取或写入操作也许 需要SSD设备上的两个不同的区块，这也减缓了SSD磁盘的性能。\n\n　　为了避免这种问题，可以采用fdisk方式来创建分区，配置三个选项来指定使用柱面及拍面大小。具体的命令如下：\n\n```sh\nfdisk -H 32 -C 32 –c /dev/sdb\n```\n# 二、配置固态硬盘(SSD)的文件系统\n\n### 1.创建文件系统\n\n　　接着需要关注的就是文件系统。想要优化文件系统删除字节区块的效率，就必须确保小于512K的文件分布在不同的删除字节区块上。要做到这一点，必须确保在创建可扩展文件系统时指定了需要使用的条带的宽度和幅度。这些值在页面中指定，默认大小为4KB。要创建一个最佳的可扩展文件系统，应该使用如下命令：\n\n```sh\nmkfs.ext4 -E stride=128,stripe-width=128 /dev/sda1\n```\n\n　　如果要修改现有的文件系统的参数，可以使用tune2fs实用程序：\n\n```sh\ntune2fs -E stride=128,stripe-width=128 /dev/sda1\n```\n\t\n### 2.文件系统日志\n\n　　关闭日志功能，可以延长SSD寿命，但是突然断电容易造成文件损坏\n\n```sh\ntune2fs -O ^has_journal /dev/sda2  关闭日志；\n```\n\n　　然后执行\n\n```sh\ne2fsck -f /dev/sda2；\n```\n\n　　检查日志是否关闭成功：\n\n```sh\ndmesg | grep EXT4\n```\n\n　　如果显示 “EXT4-fs (sda2): mounted filesystem without journal”  说明关闭日志成功；否则显示 “mounted filesystem with ordered data mode”\n\n\n　　要打开日志\n\n```sh\ntune2fs -O has_journal /dev/sda2   \n```\n\n### 3.设置noatime\n\n　　不记录文件访问时间，该选项保证了文件的访问时间不会因为每次读取而更新，从而降低对文件系统的写入次数。\n\n　　在fstb 中加入noatime　选项\n\n```sh\n/dev/sda1 / ext4 discard,defaults  改为  /dev/sda1 / ext4 noatime,defaults\n```\n\n# 三、配置固态硬盘(SSD)的I/O调度程序\n\n　　优化的第三个部分涉及到I/O调度程序。该模块是一个决定如何处理I/O请求的核心组件。默认情况下就是非常公平的排队，对于普通的磁盘驱动器来说，这是很好的方案，但对于以期限调度为优势的固态硬盘来说，这并不是最好的。\n\n　　如果你想在系统中对所有磁盘采用期限调度，可以在内核加载时把`elevator=deadline`这句话加入到系统引导管理器(GURB)中;如果你只是想针对某一个磁盘，就应该在rc.local文件中加入类似如下实例的一句话，那么每次当系统重启，期限调度就会应用到指定的磁盘。如下实例将会对 /dev/sdb磁盘采用期限调度。\n\n```sh\necho deadline > /sys/block/xvda/queue/scheduler\n```\n　　给IO的算法修改成 noop,操作系统本身不做处理,让 ssd 本身处理.\n\n```sh\necho noop >  /sys/block/sda/queue/scheduler\n```\n\n# 四、清理固态硬盘(SSD)中的数据块\n\n　　最后一个重要的步骤称为“清理”，该操作可以确保在删除文件后相应的数据块真正清空，然后在创建新的文件时才能有可用的数据块。如果没有清理操作，一旦数 据块空间填满，固态硬盘的性能就会下降。如果使用丢弃挂载选项，当文件删除后，数据块也会被相应地清除，这样可以显著提高固态硬盘的性能。2.6.33以 上的内核已经支持清理操作。\n\n　　Linux内核从2.6.33开始提供TRIM支持，所以先运行“uname -a”命令，查看自己的内核版本，如果内核版本低于2.6.33的，请先升级内核。然后运行“hdparm -I /dev/sda”查看自己的硬盘支不支持TRIM技术，如果支持，你会看到\n\n```\n* Data Set Management TRIM supported\n```\n\n　　注意：如果SSD组RAID0后，将失去Trim功能\n\n　　如果上面两个条件都满足了，就可以在fstab中添加discard来开启TRIM功能，如：\n\n```sh\n原始的UUID=2f6be0cf-2f54-4646-b8c6-5fb0aa01ef23 / ext4 defaults,errors=remount-ro 0 1\n改后的UUID=2f6be0cf-2f54-4646-b8c6-5fb0aa01ef23 / ext4 discard,defaults,errors=remount-ro,noatime 0 1\n```\n\n　　在fasab配置文件中完成对文件系统的这些修改后，重启计算机，或者通知文件系统重新读取其配置，然后使用/etc/fstab文件中包含的mount -o remount命令重新安装每个文件系统。\n\n","tags":["调优"],"categories":["system"]},{"title":"rsync安装配置实例","url":"/2017/05/26/service/rsync安装配置实例/","content":"\n本文详细介绍了rsync安装配置实例。\n<!-- more -->\n\n# 一. 安装rsync\n\n```sh\nyum install rsync\n```\n\n# 二. 配置rsync服务器端\n\n### 1、  修改rsync的配置文件\n\n```sh\ncat /etc/xinetd.d/rsync\n\n# default: off\n# description: The rsync server is a good addition to an ftp server, as it \\\n#   allows crc checksumming etc.\nservice rsync\n{\n    disable = yes\n    flags           = IPv6\n    socket_type     = stream\n    wait            = no\n    user            = root\n    server          = /usr/bin/rsync\n    server_args     = --daemon\n    log_on_failure  += USERID\n}\n\n```\n\n　　可以看到rysnc服务是关闭的(disable = yes)，这里把它开启，把disable的值改为no\n\n### 2、  创建rsync服务器配置文件/etc/rsyncd.conf\n\n```sh\nvim /etc/rsyncd.conf\n\nuid = root\ngid = root\nport = 873                                      #　指定运行端口，默认是873，您可以自己指定\nhosts allow = 192.168.0.204, 192.168.1.205      # 允许访问的客户机\n#hosts deny = 0.0.0.0/32                        #　拒绝访问的\nuse chroot = \nmax connections = \ntimeout=\n\n# 下面这些文件是安装完RSYNC服务后自动生成的文件,当然也可以手动配置到指定路径\n\npid file = /var/run/rsyncd.pid      #pid文件的存放\nlock file = /var/run/rsync.lock     #锁文件的存放位置\nlog file = /var/log/rsyncd.log      #日志记录文件的存放\nmotd file = /etc/rsyncd.motd        #欢迎\n\n# 上面这段是全局配置，下面的模块可以有\n\n[test]                                        # 模块名字，自己命名\npath = /home/hyj/workspace/test               # 指定文件目录所在位置，这是必须指定 \ncomment = rsync files                         # 注释\nignore errors                                 # 忽略IO\nread only = yes \nlist = no                                     # 是否把rsync 服务器上提供同步数据的目录显示\nauth users = rsync                            # 同步验证时用的账号，如果没有这项就是匿名同步，client同步时不用用户名也能同步。\nsecrets file = /etc/rsync.passwd              # 指定认证文件\n```\n\n### 3、  创建认证文件：\n\n#### 3.1. 创建认证文件\n```sh\nvim /etc/rsync.passwd\n\nrsync:hyl            # 用户名：密码。注意这个不是系统用户，只是rsync用户。所以不用useradd。\n```\n\n　　名字随便写，只要和上边配置文件里的“auth users”参数一致即可，格式(一行一个用户)账号：密码\n\n\n#### 3.2. 修改认证文件权限\n\n　　把认证文件的权限改成600\n\n```sh\nchmod 600 /etc/rsync.passwd          ## 只能所有者可读，否则报错\n```\n\n### 4、 欢迎信息\n\n　　如果在配置文件中指定了欢迎信息，在/etc下创建rsyncd.motd，设置欢迎信息：\n\n```sh\nvim /etc/rsyncd.motd\n\n      Welcome the rsync services!\n```\n\n# 三. 启动rsync\n\n### 1、 在server端将rsync启动：\n\n#### 1.1 启动rsync服务端（以守护进程形式，独立启动）\n\n```sh\n/usr/bin/rsync --daemon\n```\n\n#### 1.2 启动rsync服务端 （以xinetd超级进程启动）\n\n```sh\n/etc/rc.d/init.d/xinetd reload(reload是网上的说法，但是我试了一下报错，start可以)\n```\n\n### 2、 防火墙设置：\n\n　　如果服务器上装有防火墙，需在服务器中设置iptables将837端口开放。\n\n```sh\niptables -A INPUT -p tcp --dport 873 -j ACCEPT\n```\n\n# 四. 配置rsync客户端\n\n### 1、用安装服务器端的方式安装rsync。\n\n　　启动rsync，如果报如下错误，是因为在etc下没有rsyncd.conf配置文件：\n\n```sh\nrsync --daemon\nFailed to parse config file: /etc/rsyncd.conf\n```\n\n　　创建配置文件 `/etc/rsyncd.conf` 文件内容为空就行。然后启动rsync，可以启动\n\n### 2、Rsync的命令格式可以为以下六种：\n\n```sh\n　　rsync [OPTION]... SRC DEST\n　　rsync [OPTION]... SRC [USER@]HOST:DEST\n　　rsync [OPTION]... [USER@]HOST:SRC DEST\n　　rsync [OPTION]... [USER@]HOST::SRC DEST\n　　rsync [OPTION]... SRC [USER@]HOST::DEST\n　　rsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST]\n```\n\n　　常用为以下两种：\n\n#### 第一种：\n\n```sh\nrsync [OPTION]... [USER@]HOST::SRC   DEST\n```\n\n　　从远程rsync服务器中拷贝文件到本地机。当SRC路径信息包含\"::\"分隔符时启动该模式。\n\n```sh\n如：rsync -av root@172.16.78.192::www /databack\n```\n\n#### 第二种：\n\n```sh\nrsync [OPTION]... SRC   [USER@]HOST::DEST\n```\n\n　　从本地机器拷贝文件到远程rsync服务器中。当DST路径信息包含\"::\"分隔符时启动该模式。\n\n```sh\n如：rsync -av /databack root@172.16.78.192::www\n```\n\n### 3、下面为实例：\n\n　　服务器ip为192.168.8.126，客户端ip为192.168.8.122\n\n　　(1)、把服务器上的/home/hyj/workspace/test文件夹中的内容备份到客户端的/usr/local/share/rsync_backup中:\n\n```sh\n/usr/bin/rsync -vzrtopg --delete  --progress rsync@192.168.8.126::test /usr/local/share/rsync_backup\n```\n\n　　`/etc/rsyncd.conf` 中模块的内容：\n\n```sh\n[test]\npath = /home/hyj/workspace/test\ncomment = rsync files\nignore errors\nread only = yes\nlist = no\nauth users = rsync\nsecrets file = /etc/rsync.passwd\n```\n\n　　上面这个命令行中-vzrtopg里的v是verbose，z是压缩，r是recursive，topg都是保持文件原有属性如属主、时间的参数（也可以用直接用a来代替rtopg， a为 --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD）。--progress是指显示出详细的进度情况，--delete是指如果服务器端删除了这一文件，那么客户端也相应把文件删除，保持真正的一致。\n\n　　（2）、上面的命令需要在备份的时候需要输入密码，可以在客户端建立一个密码文件，在命令中把密码文件作为参数带入：\n\n```sh\nvim /etc/rsync.passwd\n\nhyl\n```\n\n　　密码文件中不用输入用户名，只需输入密码即可。\n\n\n　　这份密码文件权限属性要设得只有root可读，不然会报错，修改属性：\n\n```sh\nchmod 600 /etc/rsync.passwd\n```\n\n　　用下面这条命令，可以不输入密码：\n\n```sh\n/usr/bin/rsync -vzrtopg --delete  --progress --password-file=/etc/rsync.passwd rsync@192.168.8.126::test /usr/local/share/rsync_backup \n```\n　　(3)、 带exclude 参数\n\n　　把服务器上的/home/hyj/workspace/test文件夹中的内容备份到客户端的/usr/local/share/rsync_backup中，但不包括:res目录和default.properties文件：\n\n```sh\n/usr/bin/rsync -vzrtopg --delete --exclude \"res/\" --exclude \"default.properties\" --progress --password-file=/etc/rsync.passwd rsync@192.168.8.126::test /usr/local/share/rsync_backup \n```\n\n** exclude/include规则实例 **\n\n```sh\nHere are some exclude/include examples:\n --exclude \"*.o\"   would exclude all filenames matching *.o\n --exclude \"/foo\"  would exclude a file in the base directory called foo\n --exclude \"foo/\"  would exclude any directory called foo.\n --exclude \"/foobar\" would exclude any file called bar two or more levels below a base directory called foo.\n --include \"*/\" --include \"*.c\" --exclude \"*\" would include all directories and C source files\n--include \"foo/\" --include \"foo/bar.c\" --exclude \"*\" would include only foo/bar.c\n (the foo/ directory must be explicitly included or it would be excluded by the \"*\")\n\n```\n　　(4)、 把客户端上的/home/hyj/vitest文件夹中的内容备份到服务器的/usr/local/share/rsync_backup中，在客户端执行如下命令:\n\n```sh\n   /usr/bin/rsync -vzrtopg --delete --progress --password-file=/etc/rsync.passwd /home/hyj/vitest rsync@192.168.8.126::clientdata\n```\n\n　　此时服务器的配置文件/etc/rsyncd.conf内容为:\n\n```sh\nuid = root\ngid = root\nhosts allow = 192.168.8.122, 192.168.8.123\n#hosts deny = 0.0.0.0/32\nuse chroot = no\nmax connections = 10\npid file = /var/run/rsyncd.pid\nlock file = /var/run/rsync.lock\nlog file = /var/log/rsyncd.log\ntimeout=600\n\n[test]\npath = /home/hyj/workspace/test\ncomment = rsync files\nignore errors\nread only = yes\nlist = no\nauth users = rsync\nsecrets file = /etc/rsync.passwd\n\n # 上面的命令中，客户端的数据备份到clientdata模块中，备份到/usr/local/share/rsync_backup文件夹下，read only改为no，# # 否则会报 `ERROR: module is read only` 的错误\n\n[clientdata]\npath = /usr/local/share/rsync_backup\ncomment = rsync files\nignore errors\nread only = no\nlist = no\nauth users = rsync\nsecrets file = /etc/rsync.passwd\n```\n\n\n# FAQ\n\n### 1、我需要在防火墙上开放哪些端口以适应rsync？\n\n　　视情况而定\n\n　　rsync可以直接通过873端口的tcp连接传文件，也可以通过22端口的ssh来进行文件传递，但你也可以通过下列命令改变它的端口：\n\n```sh\nrsync --port 8730 otherhost::\n或者\nrsync -e 'ssh -p 2002' otherhost:\n```\n\n### 2、 我如何通过rsync只复制目录结构，忽略掉文件呢？\n\n```sh\nrsync -av --include '*/' --exclude '*' source-dir dest-dir\n```\n\n# 常见错误\n\n```sh\nrsync: failed to connect to 218.107.243.2: No route to host (113) \nrsync error: error in socket IO (code 10) at clientserver.c(104) [receiver=2.6.9]\n```\n　　解决：对方没开机、防火墙阻挡、通过的网络上有防火墙阻挡，都有可能。关闭防火墙，其实就是把tcp udp 的873端口打开：\n\n---\n\n```sh\npassword file must not be other-accessible \ncontinuing without password file \nPassword: \n```\n\n　　解决：这是因为rsyncd.pwd rsyncd.sec的权限不对，应该设置为600。如：`chmod 600 rsyncd.pwd`\n\n---\n\n```sh\n@ERROR: auth failed on module xxxxx \nrsync: connection unexpectedly closed (90 bytes read so far) \nrsync error: error in rsync protocol data stream (code 12) at io.c(150) \n```\n\n　　解决：这是因为密码设置错了，无法登入成功，检查一下rsync.pwd，看客服是否匹配。还有服务器端没启动rsync 服务也会出现这种情况。 \n\n---\n\n```sh\n@ERROR: chroot failed \nrsync: connection unexpectedly closed (75 bytes read so far) \nrsync error: error in rsync protocol data stream (code 12) at io.c(150) \n```\n\n　　解决：这是因为你在 rsync.conf 中设置的 path 路径不存在，要新建目录才能开启同步。 \n\n---\n\n```sh\n[root@hyj rsync_backup]# /usr/bin/rsync -vzrtopg --delete --exclude \"res/\" --exclude \"default.properties\" --progress rsync@192.168.8.126::test /usr/local/share/rsync_backup --password-file=/etc/rsync.pass\n\n@ERROR: chdir failed\n\nrsync error: error starting client-server protocol (code 5) at main.c(1516) [Receiver=3.0.9]\n```\n\n　　原因及解决办法：SELinux；（下面这条命令在服务器端执行）\n```sh\nsetsebool -P rsync_disable_trans on\n```\n\n---\n\n```sh\nERROR: module is read only\nrsync: read error: Software caused connection abort (113)\nrsync error: error in rsync protocol data stream (code 12) at io.c(769) [sender=3.0.8]\n```\n\n　　解决：这是因为服务器端配置文件rsyncd.conf中read only = yes，为只读，即不允许客户端上传文件，改成no就可以了。\n","tags":["rsync"],"categories":["service"]},{"title":"pdflush进程详解与优化","url":"/2017/05/25/system/pdflush进程详解与优化/","content":"# 一、简介\n\n　　由于页高速缓存的缓存作用，写操作实际上会被延迟。当页高速缓存中的数据比后台存储的数据更新时，那么该数据就被称做脏数据。在内存中累积起来的脏页最终必须被写回磁盘。\n\n<!-- more -->\n\n在以下两种情况发生时，脏页被写回磁盘：\n\n* 当空闲内存低于一个特定的阈值时，内核必须将脏页写回磁盘，以便释放内存。\n* 当脏页在内存中驻留时间超过一个特定的阈值时，内核必须将超时的脏页写回磁盘，以确保脏页不会无限期地驻留在内存中。\n\n　　上面两种工作的目的完全不同。实际上，在老内核中，这是由两个独立的内核线程分别完成的。但是在2.6内核中，由一群内核线程—pdflush后台回写例程—统一执行两种工作。\n\n　　我们来看看这两个目标是如何具体实现的。首先，当系统中的空闲内存低于一个特定的阈值时，pdflush线程将脏页刷新回磁盘。该后台回写例程的目的在于在可用物理内存过低时，释放脏页以重新获得内存。特定的内存阈值可以通过`dirty_background_ratio`参数设置。当空闲内存比阈值`dirty_ background_ratio`还低时，内核便会调用函数`wakeup_bdflush()`唤醒一个pdflush线程，随后pdflush线程进一步调用函数`background_writeout()`开始将脏页写回磁盘。函数`background_ writeout()`需要一个长整型参数，该参数指定试图回写的页面数目。\n\n　　函数`background_writeout()`会连续地写出数据，直到满足以下两个条件：\n\n* 已经有指定的最小数目的页被写出到磁盘。\n* 空闲内存数已经回升，超过了阈值dirty_background_ratio。\n\n　　上述条件确保了pdflush操作可以减轻系统中内存不足的压力。回写操作不会在达到这两个条件前停止，除非pdflush写回了所有的脏页，没有剩下的脏页可再被写回了。\n\n　　要满足第二个目标，pdflush后台例程会被周期性唤醒（和空闲内存是否过低无关），将那些在内存中驻留时间过长的脏页写出，确保内存中不会有长期存在的脏页。假如系统发生崩溃，则内存会处于混乱之中，而那些在内存中还没来得及写回磁盘的脏页就会丢失，所以周期性同步回写非常重要。\n\n　　在系统启动时，内核初始化一个定时器，让它周期地唤醒pdflush线程，随后使其运行函数`wb_kupdate()`。该函数将把所有驻留时间超过百分之`dirty_expire_centisecs`秒的脏页写回。然后定时器将再次被初始化为百分之`dirty_expire_ centisecs`秒后唤醒pdflush线程。\n\n　　总而言之，pdflush线程周期地被唤醒并且把超过特定期限的脏页写回磁盘。\n\n# 二、proc下的相关控制参数\n\n　　系统管理员可以在/proc/sys/vm中设置回写相关的参数，也可以通过sysctl系统调用设置它们。\n\n* /proc/sys/vm/dirty_ratio\n\n　　这个参数控制一个进程在文件系统中的文件系统写缓冲区的大小，单位是百分比，表示系统内存的百分比，表示当一个进程中写缓冲使用到系统内存多少的时候，再有磁盘写操作时开始向磁盘写出数据。增大之会使用更多系统内存用于磁盘写缓冲，也可以极大提高系统的写性能。但是，当你需要持续、恒定的写入场合时，应该降低其数值.一般缺省是 40。设置方法如下：\n\n```sh\necho 30 >/proc/sys/vm/dirty_ratio\n```\n\n---\n\n* /proc/sys/vm/dirty_background_ratio\n\n　　这个参数控制文件系统的pdflush进程，在何时刷新磁盘。单位是百分比，表示系统总内存的百分比，意思是当磁盘的脏数据缓冲到系统内存多少的时候，pdflush开始把脏数据刷新到磁盘。增大会使用更多系统内存用于磁盘写缓冲，也可以极大提高系统的写性能。但是，当你需要持续、恒定的写入场合时，应该降低其数值.一般缺省是10。设置方法如下：\n\n```sh\necho 8 >/proc/sys/vm/dirty_background_ratio\n```\n\n---\n\n* /proc/sys/vm/dirty_writeback_centisecs\n\n　　Pdflush写后台进程每隔多久被唤醒并执行把脏数据写出到硬盘。单位是 1/100 秒。如果你的系统是持续地写入动作，那么实际上还是降低这个数值比较好，这样可以把尖峰的写操作削平成多次写操作。缺省数值是500，也就是 5 秒。设置方法如下：\n\n```sh\necho 200 >/proc/sys/vm/dirty_writeback_centisecs\n```\n\n---\n\n* /proc/sys/vm/dirty_expire_centisecs\n\n　　这个参数声明Linux内核写缓冲区里面的脏数据多“旧”了之后，pdflush 进程就开始考虑写到磁盘中去。单位是 1/100秒。对于特别重载的写操作来说，这个值适当缩小也是好的，但也不能缩小太多，因为缩小太多也会导致IO提高太快。缺省是 30000，也就是 30 秒的数据就算旧了，将会刷新磁盘。建议设置为 1500，也就是15秒算旧。设置方法如下：\n\n```sh\necho 1500 >/proc/sys/vm/dirty_expire_centisecs\n```\n\n---\n\n# 三、内核参数修改后的生效\n\n　　Linux在系统运行时修改内核参数(/proc/sys与/etc/sysctl.conf)，而不需要重新引导系统，这个功能是通过/proc虚拟文件系统实现的。\n\n　　在/proc/sys目录下存放着大多数的内核参数，并且设计成可以在系统运行的同时进行更改,可以通过更改/proc/sys中内核参数对应的文件达到修改内核参数的目的(修改过后，保存配置文件就马上自动生效)，不过重新启动机器后之前修改的参数值会失效，所以只能是一种临时参数变更方案。(适合调试内核参数优化值的时候使用，如果设置值有问题，重启服务器还原原来的设置参数值了。简单方便。)\n\n　　但是如果调试内核参数优化值结束后，需要永久保存参数值，就要通过修改/etc/sysctl.conf内的内核参数来永久保存更改。但只是修改sysctl文件内的参数值，确认保存修改文件后，设定的参数值并不会马上生效，如果想使参数值修改马上生效，并且不重启服务器，可以执行下面的命令：\n\n```sh\nsysctl –p\n```\n\n---\n\n　　下面介绍一下/proc/sys下内核文件与配置文件sysctl.conf中变量的对应关系：\n\n　　由于可以修改的内核参数都在/proc/sys目录下，所以sysctl.conf的变量名省略了目录的前面部分（/proc/sys）。即将/proc/sys中的文件转换成sysctl中的变量依据下面两个简单的规则：\n\n1. 去掉前面部分/proc/sys\n2. 将文件名中的斜杠变为点\n\n　　这两条规则可以将/proc/sys中的任一文件名转换成sysctl中的变量名。\n\n　　例如：\n\n```sh\n/proc/sys/net/ipv4/ip_forward => net.ipv4.ip_forward\n/proc/sys/kernel/hostname =>  kernel.hostname\n```\n\n　　可以使用下面命令查询所有可修改的变量名\n\n```sh\nsysctl –a\n```\n","tags":["调优"],"categories":["system"]},{"title":"MongoDB归档及压缩工具","url":"/2017/05/25/database/MongoDB归档及压缩工具/","content":"　　原文地址：http://t.dbdao.com/archives/archiving-and-compression-in-mongodb-tools.html\n\n\n# 介绍\n\n　　我在MongoDB World 2015做的演讲“Putting the Go in MongoDB”，重点是关于MongoDB工具的重写，从C ++到Go，这在可用性以及性能方面得到了一些改进，但是这里我只简要的说两个方面的新功能，(planned for the 3.2 release) – 归档和压缩。\n\n　　在本文中，我将对mongodump和mongorestore提供更详细的归档和压缩特性说明，并探索使用这些特性的可行用例。\n\n\n<!-- more -->\n\n# 概述\n\n　　一个通常目的的归档一般由一个或多个文件组成。这样例子如磁带归档格式(tar)，其中包含按顺序组成的一个或多个文件。归档在执行进程间通信的应用程序中尤其有用，例如，你可以通过远程服务器进行目录的tarball压缩，然后通过SSH，传送到到本机上进行解压：\n\n```ssh\nssh source.server.com tar c sourceDirectory | tar x\n```\n\n　　由于归档以顺序的方式创建，接收端将能按顺序接收到发送端按顺序发来的数据。\n\n　　在3.0中，我们增加了在MongoDB中并发执行备份和恢复多个集合的能力，这可以让你执行备份时，更加充分地利用磁盘I / O。 结果，写入mongodump的备份并不一定以顺序的方式接收。 同样，mongorestore同时读取还原操作集合，它的读取指令也并非是序列性的。\n\n　　通用归档格式，如tar，只支持连续的文件归档打包。mongodump和mongorestore利用这些备份格式，将得到一个不可接受的性能退化， 由于所有集合的数据将不得不被按顺序写入和读出。为了支持这些工具的并发行为，我们研发了一个特殊的通用备份格式，支持非并发文件的写入。 这个新的归档特性极大了提高了备份和还原操作的效率。\n\n# 背景\n\n　　为了按上下文情况进行备份，我们考虑一下你们通常是如何创建备份的。比如，假设你有一个“country”的数据库，其中含有两个集合： “nigeria” and “austria”， 你可能会这样操作：\n\n```sh\t\nmongodump --db country\n```\n\n　　上面的指令读取“country”数据库的所有集合， 然后将其写入“dump”目录。 上面的指令就会产生以下的目录列表：\n\n```sh\ndump/\n└── [4.3M]  country\n    ├── [2.1M]  austria.bson\n    ├── [  87]  austria.metadata.json\n    ├── [2.1M]  nigeria.bson\n    ├── [  87]  nigeria.metadata.json\n    └── [ 140]  system.indexes.bson\n \n1 directory, 5 files\n```\n\n　　你也可以备份整个服务器-这里的服务器包含两个数据库(country 和product)。\n\n```sh\nmongodump\n```\n\n```sh\n├── [5.4M]  dump\n│   ├── [4.03M]  country\n│   │   ├── [2.1M]  austria.bson\n│   │   ├── [  87]  austria.metadata.json\n│   │   ├── [2.1M]  nigeria.bson\n│   │   ├── [  87]  nigeria.metadata.json\n│   │   └── [ 140]  system.indexes.bson\n│   └── [1.1M]  product\n│       ├── [1.0M]  mongodump.bson\n│       ├── [  89]  mongodump.metadata.json\n│       └── [  72]  system.indexes.bson\n2 directories, 8 files\n```\n\n　　或选择备份单个集合到标准输出，而不是一个目录：\n\n```sh\t\nmongodump --db country --collection nigeria --out -\n```\n\n# 归档支持\n\n　　在3.2中，我们引入了创建备份的一个附加模式 －－ “归档”模式，写入所有转储数据，甚至从不同的数据库和集合到单一的输出文件。 使用mongodump创建归档是极为简单的 – 只需要一个附加选项：\n\n```sh\nmongodump --db country --archive=country.archive\n\t\n-rw-rw-r-- 1 wisdom wisdom 4.2M Jun 29 11:12 country.archive\n```\n\n　　上面的指令将在“country.archive”文件中创建“country”的数据库归档。默认情况下，归档被写入到标准输出。不同于目录模式的执行备份，创建目录树，默认归档模式下备份结果就是一个单一的文件， 包含“country”数据库的所有数据-所有集合，索引等。\n\n　　你也可以备份一个单一的集合或整个服务器的内容：\n\n　　**单一集合：**\n\n```sh\t\nmongodump --db country --collection nigeria --archive=nga.archive\n\t\n-rw-rw-r-- 1 wisdom wisdom 2.1M Jun 29 11:15 nga.archive\n```\n\n　　**整个服务器：**\n\n```sh\nmongodump --archive=server.archive\n\t\n-rw-rw-r-- 1 wisdom wisdom 5.3M Jun 29 11:26 server.archive\n```\n\n　　在mongodump的这种情况下，归档模式允许多个集合以非连续的方式打包在归档文件内。在mongorestore中，它允许多个集合进行并行恢复。这样，你可以在网络上执行数据迁移，降低磁盘I/O所占空间，享受到充分利用工具和底层存储引擎的并发所带来的好处。\n\n# 数据迁移\n\n　　一个新的备份改善的例子， 是mongodump和mongorestore之间的进程间通信 – 特别是能够将数据从一个传到另一个。在以前的版本中，这种支持有限 – 一个时间只能传输一个集合。现在，使用归档就没有这样的限制。这种方式对于数据库服务器出于安全考虑而安装有防火墙的情况下很有用。在这种情况下，一个通常的设计是允许一个或多个服务器方访问数据库。使用归档功能，在SSH上进行数据转移数据，就轻而易举了：\n\n```sh\nssh wisdom@proxy.server.com mongodump --host source.server.com --archive  | ssh wisdom@target.server.com mongorestore --archive\n```\n\n　　上面的指令使用SSH方式连接到代理主机（proxy.server.com），访问源服务器（source.server.com），在代理服务器上运行mongodump，为了最终的恢复，将源服务器的发送内容（通过SSH）到目标服务器（target.server.com）。\n\n　　如果没有归档，通过mongodump完成这些操作的唯一办法就是，先执行备份到磁盘，在将文件复制到目标服务器，然后运行mongorestore。通过备份，一个指令就可以完成- 无需任何附加磁盘I/O的开销。\n\n# 压缩支持\n\n　　除了备份，我们还使用gzip进行压缩。这是通过在mongodump和mongorestore中引入一个新的指令行选项 “--gzip” 实现的。 压缩可用于目录以及归档模型下创建的备份，压缩还可以减少磁盘空间使用。\n\n```sh\nmongodump --db country --gzip\n```\n\n　　生成：\n\n```sh\ndump/\n└── [568K]  country\n    ├── [254K]  austria.bson.gz\n    ├── [ 100]  austria.metadata.json.gz\n    ├── [254K]  nigeria.bson.gz\n    ├── [ 100]  nigeria.metadata.json.gz\n    └── [  91]  system.indexes.bson.gz\n \n1 directory, 5 files\n```\n\n　　注意,目录模型的归档备份大小-568KB-比没有压缩的备份要小很多-4.3MB.\n\n　　**压缩归档：**\n\n```sh\nmongodump --db country --gzip --archive=country.archive\n\n-rw-rw-r-- 1 wisdom wisdom 509K Jun 29 11:23 country.archive\n```\n\n　　对于归档来说，数据在写入归档之前需要先压缩。\n\n　　恢复压缩目录模式备份，你应该运行：\n\n```sh\t\nmongorestore --gzip\n```\n\n　　类似用来恢复归档模式下的压缩备份的命令：\n\n```sh\t\nssh wisdom@proxy.server.com mongodump --host source.server.com --archive --gzip  | ssh wisdom@target.server.com mongorestore --archive --gzip\n```\n\n　　数据迁移不会产生任何磁盘I / O开销，由于压缩，将会使用更少的网络带宽。\n\n# 总结\n\n　　归档和压缩特性产生了许多用于进行备份和恢复操作的例子。如果你们正在使用MongoDB工具和其它类型的应用程序，我们也乐于倾听你们的经验及用例。 尽管目前最新版本工具还不文档，不过希望大家先对这些特性体验起来。\n\n　　**注：** 作为提供共享集群的集群范围快照的唯一备份解决方案，MongoDB Ops Manager和MongoDB Cloud Mannager被推荐用于较大的MongoDB部署。\n","tags":["mongodb"],"categories":["database"]},{"title":"Linux下清空或删除大文件内容的5种方法","url":"/2017/05/25/system/Linux下清空或删除大文件内容的5种方法/","content":"\n\n编译自：http://www.tecmint.com/empty-delete-file-content-linux/ 作者： Aaron Kili\n\n原创：LCTT https://linux.cn/article-8024-1.html 译者： FSSlc \n\n在 Linux 终端下处理文件时，有时我们想直接清空文件的内容但又不必使用任何 Linux 命令行编辑器 去打开这些文件。那怎样才能达到这个目的呢？在这篇文章中，我们将介绍几种借助一些实用的命令来清空文件内容的方法。\n\n**注意：** 在我们进一步深入了解这些方法之前，请记住: 由于在 Linux 中一切皆文件，你需要时刻注意，确保你将要清空的文件不是重要的用户文件或者系统文件。清空重要的系统文件或者配置文件可能会引发严重的应用失败或者系统错误。\n\n前面已经说道，下面的这些方法都是从命令行中达到清空文件的目的。\n\n**提示：** 在下面的示例中，我们将使用名为 access.log 的文件来作为示例样本。\n<!-- more -->\n\n# 1. 通过重定向到 Null 来清空文件内容\n\n清空或者让一个文件成为空白的最简单方式，是像下面那样，通过 shell 重定向 `null` （不存在的事物）到该文件：\n\n```sh\n> access.log\n```\n\n```sh\n[root@localhost logs]# du -sh catalina.out \n9.7G\tcatalina.out\n[root@localhost logs]# > catalina.out \n[root@localhost logs]# du -sh catalina.out \n0\tcatalina.out\n```\n\n在 Linux 下使用 Null 重定向来清空大文件\n\n# 2. 使用 `true` 命令重定向来清空文件\n\n下面我们将使用 : 符号，它是 shell 的一个内置命令，等同于 true 命令，它可被用来作为一个 no-op（即不进行任何操作）。\n\n另一种清空文件的方法是将 : 或者 true 内置命令的输出重定向到文件中，具体如下：\n\n```sh\n: > access.log\n```\n 或\n```sh\ntrue > access.log\n```\n\n使用 Linux 命令清空大文件\n\n# 3. 使用 cat/cp/dd 实用工具及 /dev/null 设备来清空文件\n\n在 Linux 中， null 设备基本上被用来丢弃某个进程不再需要的输出流，或者作为某个输入流的空白文件，这些通常可以利用重定向机制来达到。\n\n所以 /dev/null 设备文件是一个特殊的文件，它将清空送到它这里来的所有输入，而它的输出则可被视为一个空文件。\n\n另外，你可以通过使用 cat 命令 显示 /dev/null 的内容然后重定向输出到某个文件，以此来达到清空该文件的目的。\n\n```sh\ncat /dev/null > access.log\n```\n\n使用 cat 命令来清空文件\n\n下面，我们将使用 cp 命令 复制 /dev/null 的内容到某个文件来达到清空该文件的目的，具体如下所示：\n\n```sh\ncp /dev/null access.log\n```\n\n\n使用 cp 命令来清空文件\n\n而下面的命令中， if 代表输入文件，of 代表输出文件。\n\n```sh\ndd if=/dev/null of=access.log\n```\n\n\n使用 dd 命令来清空文件内容\n\n# 4. 使用 echo 命令清空文件\n\n在这里，你可以使用 echo 命令 将空字符串的内容重定向到文件中，具体如下：\n\n```sh\necho \"\" > access.log\n```\n或者\n ```sh\n echo > access.log\n```\n\n使用 echo 命令来清空文件\n\n**注意：**你应该记住空字符串并不等同于 null 。字符串表明它是一个具体的事物，只不过它的内容可能是空的，但 null 则意味着某个事物并不存在。\n\n基于这个原因，当你将 echo 命令 的输出作为输入重定向到文件后，使用 cat 命令 来查看该文件的内容时，你将看到一个空白行（即一个空字符串）。\n\n要将 null 做为输出输入到文件中，你应该使用 -n 选项，这个选项将告诉 echo 不再像上面的那个命令那样输出结尾的那个新行。\n\n```sh\necho -n \"\" > access.log\n```\n\n使用 Null 重定向来清空文件\n\n# 5. 使用 truncate 命令来清空文件内容\n\ntruncate 可被用来将一个文件缩小或者扩展到某个给定的大小。\n\n你可以利用它和 -s 参数来特别指定文件的大小。要清空文件的内容，则在下面的命令中将文件的大小设定为 0:\n\n```sh\ntruncate -s 0 access.log\n```\n\n在 Linux 中截断文件内容\n\n我要介绍的就是这么多了。在本文中，我们介绍了几种通过使用一些简单的命令行工具和 shell 重定向机制来清除或清空文件内容的方法。\n\n上面介绍的这些可能并不是达到清空文件内容这个目的的所有可行的实践方法，所以你也可以通过下面的评论栏告诉我们本文中尚未提及的其他方法。\n\n\n---\n\nvia: http://www.tecmint.com/empty-delete-file-content-linux/\n\n作者：Aaron Kili 译者：FSSlc 校对：jasminepeng\n\n本文由 LCTT 原创编译，Linux中国 荣誉推出\n","tags":["bash"],"categories":["system"]},{"title":"redis模糊删除key","url":"/2017/05/25/database/redis模糊删除key/","content":"\nRedis keys命令支持模式匹配，但是del命令不支持模式匹配，有时候需要根据一定的模式来模糊删除key，这时只能结合shell命令来完成了。 具体命令是：\n\n<!-- more -->\n\n```sh\nredis-cli KEYS \"pattern\" | xargs redis-cli DEL\n```\n\n其中pattern是keys命令支持的模式，这样就可以模糊删除key了。服务器上测试删除150万条数据的效率也是很高的。\n\n所有的Redis命令可以在这里找到：http://redis.io/commands\n\nKEYS命令：http://redis.io/commands/keys\n\nDEL命令: http://redis.io/commands/del\n\n**my demo:**\n\nprefix_: 需要删除key的匹配的前缀名\n```sh\nredis-cli KEYS \"prefix_\" | xargs redis-cli DEL \n```\n","tags":["redis"],"categories":["database"]},{"title":"Linux查看文件编码格式及文件编码转换","url":"/2017/05/25/system/Linux查看文件编码格式及文件编码转换/","content":"\n如果你需要在Linux 中操作windows下的文件，那么你可能会经常遇到文件编码转换的问题。Windows中默认的文件格式是GBK(gb2312)，而Linux一般都是UTF-8。下面介绍一下，在Linux中如何查看文件的编码及如何进行对文件进行编码转换。\n\n<!-- more -->\n\n# 查看文件编码\n\n在Linux中查看文件编码可以通过以下几种方式：\n\n### 1.在Vim 中可以直接查看文件编码\n\n```sh\n    :set fileencoding  \n```\n\n即可显示文件编码格式。\n\n如果你只是想查看其它编码格式的文件或者想解决用Vim查看文件乱码的问题，那么你可以在 `~/.vimrc` 文件中添加以下内容：\n\n```sh\n    set encoding=utf-8 fileencodings=ucs-bom,utf-8,cp936\n```\n\n这样，就可以让vim自动识别文件编码（可以自动识别UTF-8或者GBK编码的文件），其实就是依照 fileencodings提供的编码列表尝试，如果没有找到合适的编码，就用latin-1(ASCII)编码打开。\n\n### 2. enca (如果你的系统中没有安装这个命令，可以用sudo yum install -y enca 安装 )查看文件编码\n\n```sh\n$ enca filename\nfilename: Universal transformation format 8 bits; UTF-8\nCRLF line terminators\n```\n\n需要说明一点的是，enca对某些GBK编码的文件识别的不是很好，识别时会出现：\n\n```\nUnrecognized encoding\n```\n\n# 文件编码转换\n\n### 1.在Vim中直接进行转换文件编码,比如将一个文件转换成utf-8格式\n\n```sh\n    :set fileencoding=utf-8  \n```\n\n### 2. enconv 转换文件编码，比如要将一个GBK编码的文件转换成UTF-8编码，操作如下\n\n```sh\nenconv -L zh_CN -x UTF-8 filename\n```\n\n### 3. iconv 转换，iconv的命令格式如下：\n\n```sh\niconv -f encoding -t encoding inputfile\n```\n\n比如将一个UTF-8 编码的文件转换成GBK编码\n\n```sh\niconv -f GBK -t UTF-8 file1 -o file2\n```\n","tags":["字符集"],"categories":["system"]},{"title":"Bash历史中执行过的每一项命令设置时间和日期.md","url":"/2017/05/25/system/Bash历史中执行过的每一项命令设置时间和日期/","content":"\n在默认情况下，所有通过 Bash 在命令行中执行过的命令都被存储在历史缓存区或者一个叫做 ` ~/.bash_history` 的文件里。这意味着系统管理员可以看到系统上用户执行过的命令清单，或者用户可以通过像 `history` 命令这样的选项来看他或她自己的命令历史。\n<!-- more -->\n\n```sh\n[root@l-webdb-docker-dev ~]# history \n    1  vim /gotwo_data/scripts/cronjob/sync_mysql_online.sh\n    2  exit\n    3  ps -ef | grep 4004\n    4  exit\n    5  mysql\n    6  mysqldump -uroot -p db_ad > /tmp/db_ad.sql\n    7  vim /tmp/db_ad.sql \n    8  mysqldump -uroot -p db_ad > /tmp/db_ad.sql\n```\n\n从上面` history `命令的输出可知，命令被执行的日期和时间并没有显示出来。基本上所有的 Linux 发行版的默认设置都是这样的。\n\n在这篇文章里，我们将解释当在 Bash 中执行` history `命令显示每个命令时，如何配置显示时间戳信息。\n\n每个命令相关的日期和时间可以记录到历史文件中，用 `HISTTIMEFORMAT` 环境变量的设置作为命令历史的备注记录。\n\n这里有两种可行的方式来达到目的：一种是暂时的效果，一种是永久的效果。\n\n要临时设置 `HISTTIMEFORMAT `环境变量，在命令行这样输出它：\n```sh\n $ export HISTTIMEFORMAT='%F %T '\n```\n\n在上面的输出命令当中，时间戳格式如下：\n\n1. `％F`－展开为完整日期，即` ％Y-％m-％d`（年-月-日）。\n\n2. `％T`－展开为时间，即` ％H:％M:％S`（时:分:秒）。\n\n通读 date 命令的 man 手册来获得更多使用说明：\n\n```sh\nman date\n```\n\n（LCTT 译注：注意：这个功能只能用在当 HISTTIMEFORMAT 这个环境变量被设置之后，之后的那些新执行的 bash 命令才会被打上正确的时间戳。在此之前的所有命令，都将会显示成设置 HISTTIMEFORMAT 变量的时间。）\n\n然而，如果你想永久地配置该变量，用你最喜欢的编辑器打开文件 ` ~/.bashrc`\n\n```sh\n    $ vi ~/.bashrc\n```\n    \n然后在下方添加（用注释将其标记为你自己的配置）：\n  \n```sh\n# 我的配置\nexport HISTTIMEFORMAT='%F %T '\n```\n\n保存文件并退出，然后，运行下面的命令以便改动当即生效：\n\n```sh\nsource ~/.bashrc\n```\n","tags":["bash"],"categories":["system"]}]